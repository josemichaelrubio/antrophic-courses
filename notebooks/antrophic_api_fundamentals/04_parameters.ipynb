{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create an instance of the Anthropic class\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Tokens\n",
    "\n",
    "There are 3 required parameters that we must include every time we make a request to Claude:\n",
    "* `model`\n",
    "* `max_tokens`\n",
    "* `messages`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "**tokens** =  a series of word-fragments that most LLMs use as building blocks (Most Large Language Models don't \"think\" in full words). These small building blocks of a text sequence that Claude processes, understands, and generates texts with. When we provide a prompt to Claude, that prompt is first turned into tokens and passed to the model. The model then begins generating its output one token at a time.\n",
    "\n",
    "For Claude, ***a token approximately represents 3.5 English characters***, though the exact number can vary depending on the language used.\n",
    "\n",
    "#### `max_tokens` \n",
    "`max_tokens` =  controls the maximum number of tokens that Claude should generate in its response\n",
    "\n",
    "The `max_tokens` parameter allows us to set an upper limit on how many tokens Claude generates for us.\n",
    "\n",
    "Setting the `max_tokens` number too low will cause Claude to immediately stop as soon as it hits 10 tokens. This will often lead to truncated or incomplete outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a poem for you:\n",
      "\n",
      "Golden sun\n"
     ]
    }
   ],
   "source": [
    "# Example of `max_tokens` parameter set to 10\n",
    "truncated_response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=10,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "print(truncated_response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the `stop_reason` property on the response Message object to see WHY the model stopped generating. In this case, we can see that it has a value of \"max_tokens\" which tells us the model stopped generating because it hit our max token limit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_tokens'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_response.stop_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a poem for you:\n",
      "\n",
      "The Whisper of the Wind\n",
      "\n",
      "Gently, the breeze caresses the leaves,\n",
      "Whispering secrets, a melody it weaves.\n",
      "Dancing shadows play upon the ground,\n",
      "As the wind's soft voice makes not a sound.\n",
      "\n",
      "It brushes my cheek, so light and free,\n",
      "Carrying messages only nature can see.\n",
      "The rustling trees join the serenade,\n",
      "A symphony of peace, nature's own shade.\n",
      "\n",
      "The wind's quiet call, a tranquil refrain,\n",
      "Soothes the soul, eases the mind's strain.\n",
      "In this moment, time seems to stand still,\n",
      "As I listen, entranced, to nature's sweet thrill.\n"
     ]
    }
   ],
   "source": [
    "# Example of `max_tokens` parameter set to 500\n",
    "longer_poem_response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n",
    "    ]\n",
    ")\n",
    "print(longer_poem_response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the `stop_reason` for this response, we'll see a value of \"end_turn\" which is the model's way of telling us that it naturally finished generating. It wrote us a poem and had nothing else to say, so it stopped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_turn'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_poem_response.stop_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: ***Models don't \"know\" about `max_tokens` when generating content.***\n",
    "\n",
    "Changing `max_tokens` won't alter how Claude generates the output, it just gives the model room to keep generating (with a high `max_tokens` value) or truncates the output (with a low `max_tokens` value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: ***Increasing `max_tokens` does not ensure that Claude actually generates a specific number of tokens.*** If we ask Claude to write a joke and set max_tokens to 1000, we'll almost certainly get a response that is much shorter than 1000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of `max_tokens` parameter set to 1000\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a silly joke for you:\n",
      "\n",
      "Why was the math book sad?\n",
      "Because it had too many problems!\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Only 30 tokens were used (when I ran it at this instance)\n",
    "print(response.usage.output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
